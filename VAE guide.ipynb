{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE guide\n",
    "\n",
    "In this tutorial you'll learn how to make your very own variational autoencoder (VAE).\n",
    "We'll be using the [Keras functional API](https://keras.io/getting-started/functional-api-guide/). The original paper describing the VAE, can be found [here](https://arxiv.org/abs/1312.6114)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "\n",
    "First, we need to get an idea of whan an autoencoder is. The goal of an autoencoder is to compress data, yet keeping the information describing the data. The autoencoder will implicitly extract the most descriptive components of the data when compressing it, and it is therefore a commonly used component in creative architectures within DL. The idea behind an autoencoder is relatively simple - push raw data (X) through a shrinking pipeline (q) and make the autoencoder network learn what features to remove and what to keep. Push the shrunken data (z) through an increasing sized pipeline (p), and make the autoencoder network learn what features it needs to add to reproduce the raw input, as shown in the figure below.\n",
    "\n",
    "<img src=\"images/autoencoder_Jwaaler.png\" title=\"Auoencoder architecture\" width=\"500\"/>\n",
    "\n",
    "#### Variational autoencoder\n",
    "\n",
    "Let's think probability. The joint probability of the model above is:\n",
    "\n",
    "$$p(X,z) = p (X | z)p(z)$$\n",
    "\n",
    "According to Bayes: $$p(z | X) = \\frac{p(X | z)p(z)}{p(X)}$$\n",
    "\n",
    "Which can be calculated by marginalizing the latent varables:\n",
    "\n",
    "$$p(X) = \\int p(X | z)p(z)dz $$\n",
    "\n",
    "Which is hard to calculate, as it requires exponentional computational time over all configurations of the latent variables.\n",
    "\n",
    "This is why we approximate $p(z | X)$ with a family of distributions $q_{\\lambda}(z | X)$, where the variational parameter $\\lambda$ represents the family of distributions. To calulate the amount of lost information when approximating $p(z | X)$ using $q(z | X)$, [Kullback-Leibler divergence](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68) is used (which is impossible to compute directly, hence the use of [Evidence Lower BOund](http://edwardlib.org/tutorials/klqp)).\n",
    "\n",
    "I highly recommend reading [this](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) before you continue. Intuetively, the model is tasked to learn the distribution of the input data.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "First, let's create a function that samples from the latent space, given mean ($\\mu$) and variance ($\\sigma^2$) as arguments. We sample from an isotropic Gaussian distribution so we can train the network. This process is called the reparameteriazation trick, which you can read about [here](https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important). Instead of sampling from $q(z | X)$, we sample $\\epsilon = N(0,1)$, so that $z = \\mu + \\sqrt{\\sigma^2}\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def sampling(input_tensor):\n",
    "    z_mean, z_log_var = input_tensor\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some parameters for the model\n",
    "params = {\n",
    "    'filters': 8, # How many filters should the first convolutional layer contain?\n",
    "    'hidden_layers': 4, # How many hidden layers should the encoder and the decoder contain?\n",
    "    'latent_dim': 4, # How many distributions should the VAE learn?\n",
    "    'kernel_size': 3, # How big should the kernels be?\n",
    "    'input_shape': (128, 128, 1), # What is the input-shape?\n",
    "    'last_activation': 'sigmoid' # What should be the last activation?\n",
    "}\n",
    "\n",
    "params['filters'] //= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Input,\n",
    "                          Dense,\n",
    "                          Conv2D,\n",
    "                          Flatten,\n",
    "                          Lambda)\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def encoder(input_shape,\n",
    "            filters,\n",
    "            hidden_layers,\n",
    "            latent_dim,\n",
    "            kernel_size,\n",
    "            **kwargs) -> \"Model\":\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = inputs\n",
    "\n",
    "    # The encoder is \"shrinking\" the input by stride = 2.\n",
    "    # The number of filters is increasing by a factor of two for each hidden layer\n",
    "    for i in range(hidden_layers):\n",
    "        filters *= 2\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   activation='relu',\n",
    "                   strides=2,\n",
    "                   padding='same',\n",
    "                   name='encoder_conv_{}'.format(i))(x)\n",
    "        \n",
    "        \n",
    "\n",
    "    intermediate_shape = K.int_shape(x)\n",
    "    \n",
    "    # The latent vector is created (q(z | X))\n",
    "    x = Flatten(name='encoder_flatten')(x)\n",
    "    x = Dense(latent_dim*8, activation='relu', name='encoder_intermediate_layer')(x)\n",
    "    z_mean = Dense(latent_dim, name='encoder_z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='encoder_z_log_var')(x)\n",
    "    \n",
    "    # Sample z\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='encoder_z')([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    params['filters'] = filters\n",
    "    return encoder, intermediate_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Conv2DTranspose,\n",
    "                          Reshape)\n",
    "\n",
    "def decoder(prev,\n",
    "            intermediate_shape,\n",
    "            hidden_layers,\n",
    "            latent_dim,\n",
    "            last_activation,\n",
    "            kernel_size,\n",
    "            **kwargs) -> \"Model\":\n",
    "    filters = intermediate_shape[3]\n",
    "    # Upsampling and reshaping to intermediate shape\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='decoder_sampled_input')\n",
    "    x = Dense(intermediate_shape[1] * intermediate_shape[2] * intermediate_shape[3],\n",
    "              activation='relu',\n",
    "              name='decoder_intermediate')(latent_inputs)\n",
    "    x = Reshape((intermediate_shape[1], intermediate_shape[2], intermediate_shape[3]),\n",
    "                name='decoder_reshape')(x)\n",
    "    \n",
    "    for i in range(hidden_layers):\n",
    "        \n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='relu',\n",
    "                            strides=2,\n",
    "                            padding='same',\n",
    "                            name='decoder_conv_{}'.format(i))(x)\n",
    "        \n",
    "        filters //= 2\n",
    "        \n",
    "    outputs = Conv2DTranspose(filters=1,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation=last_activation,\n",
    "                              padding='same',\n",
    "                              name='decoder_output')(x)\n",
    "\n",
    "    # Make model object\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    \n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/miniconda3/envs/hackathon/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "ENCODER\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 128, 128, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)         (None, 64, 64, 8)    80          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)         (None, 32, 32, 16)   1168        encoder_conv_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)         (None, 16, 16, 32)   4640        encoder_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)         (None, 8, 8, 64)     18496       encoder_conv_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_flatten (Flatten)       (None, 4096)         0           encoder_conv_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_intermediate_layer (Den (None, 32)           131104      encoder_flatten[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_z_mean (Dense)          (None, 4)            132         encoder_intermediate_layer[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "encoder_z_log_var (Dense)       (None, 4)            132         encoder_intermediate_layer[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "encoder_z (Lambda)              (None, 4)            0           encoder_z_mean[0][0]             \n",
      "                                                                 encoder_z_log_var[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 155,752\n",
      "Trainable params: 155,752\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "DECODER\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_sampled_input (Input (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "decoder_intermediate (Dense) (None, 4096)              20480     \n",
      "_________________________________________________________________\n",
      "decoder_reshape (Reshape)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_0 (Conv2DTransp (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_1 (Conv2DTransp (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "decoder_conv_2 (Conv2DTransp (None, 64, 64, 16)        4624      \n",
      "_________________________________________________________________\n",
      "decoder_conv_3 (Conv2DTransp (None, 128, 128, 8)       1160      \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       73        \n",
      "=================================================================\n",
      "Total params: 81,729\n",
      "Trainable params: 81,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "VAE\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 4), (None, 4), (N 155752    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 128, 128, 1)       81729     \n",
      "=================================================================\n",
      "Total params: 237,481\n",
      "Trainable params: 237,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Instatiating the models - gluing it all together\n",
    "encoder_model, intermediate_shape = encoder(**params)\n",
    "decoder_model = decoder(encoder_model.outputs[2], intermediate_shape, **params)\n",
    "\n",
    "outputs = decoder_model(encoder_model(encoder_model.inputs)[2])\n",
    "\n",
    "vae = Model(encoder_model.inputs, outputs, name = 'vae')\n",
    "\n",
    "print('ENCODER')\n",
    "print(encoder_model.summary())\n",
    "\n",
    "print('\\nDECODER')\n",
    "print(decoder_model.summary())\n",
    "\n",
    "print('\\nVAE')\n",
    "print(vae.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import mse\n",
    "\n",
    "# Adding loss - both (scaled) reconstruction and KL\n",
    "\n",
    "reconstruction_loss = mse(K.flatten(encoder_model.inputs), K.flatten(outputs))\n",
    "reconstruction_loss *= params['input_shape'][0]*params['input_shape'][0]\n",
    "kl_loss = 1. + encoder_model.outputs[1] - K.square(encoder_model.outputs[0]) - K.exp(encoder_model.outputs[1])\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
