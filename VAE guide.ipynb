{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE guide\n",
    "\n",
    "In this tutorial you'll learn how to make your very own variational autoencoder (VAE).\n",
    "We'll be using Python 3, the [Keras functional API](https://keras.io/getting-started/functional-api-guide/) and numpy. The original paper describing the VAE, can be found [here](https://arxiv.org/abs/1312.6114)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder\n",
    "\n",
    "First, we need to get an idea of whan an autoencoder is. The goal of an autoencoder is to compress data, yet keeping the information describing the data. The autoencoder will implicitly extract the most descriptive components of the data when compressing it, and it is therefore a commonly used component in creative architectures within DL. The idea behind an autoencoder is relatively simple - push raw data (X) through a shrinking pipeline (q) and make the autoencoder network learn what features to remove and what to keep. Push the shrunken data (z) through an increasing sized pipeline (p), and make the autoencoder network learn what features it needs to add to reproduce the raw input, as shown in the figure below.\n",
    "\n",
    "<img src=\"images/autoencoder_Jwaaler.png\" title=\"Auoencoder architecture\" width=\"500\"/>\n",
    "\n",
    "#### Variational autoencoder\n",
    "\n",
    "Let's think probability. The joint probability of the model above is:\n",
    "\n",
    "$$p(X,z) = p (X | z)p(z)$$\n",
    "\n",
    "According to Bayes: $$p(z | X) = \\frac{p(X | z)p(z)}{p(X)}$$\n",
    "\n",
    "Which can be calculated by marginalizing the latent varables:\n",
    "\n",
    "$$p(X) = \\int p(X | z)p(z)dz $$\n",
    "\n",
    "Which is hard to calculate, as it requires exponentional computational time over all configurations of the latent variables.\n",
    "\n",
    "This is why we approximate $p(z | X)$ with a family of distributions $q_{\\lambda}(z | X)$, where the variational parameter $\\lambda$ represents the family of distributions. To calulate the amount of lost information when approximating $p(z | X)$ using $q(z | X)$, [Kullback-Leibler divergence](https://towardsdatascience.com/demystifying-kl-divergence-7ebe4317ee68) is used (which is impossible to compute directly, hence the use of [Evidence Lower BOund](http://edwardlib.org/tutorials/klqp)).\n",
    "\n",
    "I highly recommend reading [this](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) before you continue. Intuetively, the model is tasked to learn the distribution of the input data.\n",
    "\n",
    "### Data\n",
    "Before we continue, we need some data. The data consists of axial, coronal and sagital MR images of the human brain. I've created a script that downloads the data for you. I've also made some functions for loading the data as it is stored as .mat files. Thank you, [Jun Cheng](https://github.com/chengjun583/), for making the data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% done.\tETA: 0 minutes\n"
     ]
    }
   ],
   "source": [
    "from utils.data_handler import download_data\n",
    "\n",
    "# First, we download the data - This might take som time. Go grab a coffee\n",
    "download_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting\n"
     ]
    }
   ],
   "source": [
    "from utils.data_handler import extract_data\n",
    "\n",
    "# Now, lets unzip the data\n",
    "extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3049, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "from utils.data_handler import load_data\n",
    "\n",
    "# Let's load the data\n",
    "# For this guide, we dont really need test and validation data\n",
    "# we just want to learn the distribution of the data\n",
    "\n",
    "x_train = load_data()\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. You should now have 3049 images with the size 512x512 stored in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "First, let's create a function that samples from the latent space, given mean ($\\mu$) and variance ($\\sigma^2$) as arguments. We sample from an isotropic Gaussian distribution so we can train the network. This process is called the reparameteriazation trick, which you can read about [here](https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important). Instead of sampling from $q(z | X)$, we sample $\\epsilon = N(0,1)$, so that $z = \\mu + \\sqrt{\\sigma^2}\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def sampling(input_tensor):\n",
    "    z_mean, z_log_var = input_tensor\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set some parameters for the model\n",
    "params = {\n",
    "    'filters': 8, # How many filters should the first convolutional layer contain?\n",
    "    'latent_dim': 16, # How many distributions should the VAE learn?\n",
    "    'kernel_size': 3, # How big should the kernels be?\n",
    "    'input_shape': x_train.shape[1:], # What is the input-shape?\n",
    "    'last_activation': 'sigmoid' # What should be the last activation?\n",
    "}\n",
    "\n",
    "params['filters'] //= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Input,\n",
    "                          Dense,\n",
    "                          Conv2D,\n",
    "                          Flatten,\n",
    "                          Lambda)\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "def encoder(input_shape,\n",
    "            filters,\n",
    "            latent_dim,\n",
    "            kernel_size,\n",
    "            **kwargs) -> \"Model\":\n",
    "    \n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "    x = inputs\n",
    "\n",
    "    # The encoder is \"shrinking\" the input by stride = 2.\n",
    "    # The number of filters is increasing by a factor of two for each hidden layer\n",
    "    for i in range(4):\n",
    "        filters *= 2\n",
    "        x = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   activation='relu',\n",
    "                   strides=2,\n",
    "                   padding='same',\n",
    "                   name='encoder_conv_{}'.format(i))(x)\n",
    "\n",
    "    intermediate_shape = K.int_shape(x)\n",
    "    \n",
    "    # The latent vector is created (q(z | X))\n",
    "    x = Flatten(name='encoder_flatten')(x)\n",
    "    x = Dense(latent_dim*8, activation='relu', name='encoder_intermediate_layer')(x)\n",
    "    z_mean = Dense(latent_dim, name='encoder_z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='encoder_z_log_var')(x)\n",
    "    \n",
    "    # Sample z\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='encoder_z')([z_mean, z_log_var])\n",
    "    \n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "    params['filters'] = filters\n",
    "    return encoder, intermediate_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Conv2DTranspose,\n",
    "                          Reshape)\n",
    "\n",
    "\n",
    "def decoder(prev,\n",
    "            intermediate_shape,\n",
    "            latent_dim,\n",
    "            last_activation,\n",
    "            kernel_size,\n",
    "            **kwargs) -> \"Model\":\n",
    "    \n",
    "    filters = intermediate_shape[3]\n",
    "    intermediate_size = intermediate_shape[1] * intermediate_shape[2] * intermediate_shape[3]\n",
    "    \n",
    "    # Upsampling and reshaping to intermediate shape\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='decoder_sampled_input')\n",
    "    \n",
    "    x = Dense(intermediate_size,\n",
    "              activation='relu',\n",
    "              name='decoder_intermediate')(latent_inputs)\n",
    "    \n",
    "    x = Reshape((intermediate_shape[1:4]),\n",
    "                name='decoder_reshape')(x)\n",
    "    \n",
    "    for i in range(4):\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            activation='relu',\n",
    "                            strides=2,\n",
    "                            padding='same',\n",
    "                            name='decoder_conv_{}'.format(i))(x)\n",
    "        \n",
    "        filters //= 2\n",
    "    \n",
    "    x = Conv2D(filters=filters//2,\n",
    "               kernel_size=kernel_size,\n",
    "               activation='relu',\n",
    "               padding='same',\n",
    "               name= 'decoder_conv{}'.format(4))(x)\n",
    "    \n",
    "    outputs = Conv2D(filters=1,\n",
    "                     kernel_size=kernel_size,\n",
    "                     activation=last_activation,\n",
    "                     padding='same',\n",
    "                     name='decoder_output')(x)\n",
    "\n",
    "    # Make model object\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    return decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/miniconda3/envs/hackathon/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "ENCODER\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 512, 512, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_0 (Conv2D)         (None, 256, 256, 8)  80          encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_1 (Conv2D)         (None, 128, 128, 16) 1168        encoder_conv_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_2 (Conv2D)         (None, 64, 64, 32)   4640        encoder_conv_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_conv_3 (Conv2D)         (None, 32, 32, 64)   18496       encoder_conv_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_flatten (Flatten)       (None, 65536)        0           encoder_conv_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_intermediate_layer (Den (None, 128)          8388736     encoder_flatten[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_z_mean (Dense)          (None, 16)           2064        encoder_intermediate_layer[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "encoder_z_log_var (Dense)       (None, 16)           2064        encoder_intermediate_layer[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "encoder_z (Lambda)              (None, 16)           0           encoder_z_mean[0][0]             \n",
      "                                                                 encoder_z_log_var[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 8,417,248\n",
      "Trainable params: 8,417,248\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "DECODER\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_sampled_input (Input (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "decoder_intermediate (Dense) (None, 65536)             1114112   \n",
      "_________________________________________________________________\n",
      "decoder_reshape (Reshape)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "decoder_conv_0 (Conv2DTransp (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "decoder_conv_1 (Conv2DTransp (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "decoder_conv_2 (Conv2DTransp (None, 256, 256, 16)      4624      \n",
      "_________________________________________________________________\n",
      "decoder_conv_3 (Conv2DTransp (None, 512, 512, 8)       1160      \n",
      "_________________________________________________________________\n",
      "decoder_conv4 (Conv2D)       (None, 512, 512, 2)       146       \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2D)      (None, 512, 512, 1)       19        \n",
      "=================================================================\n",
      "Total params: 1,175,453\n",
      "Trainable params: 1,175,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "VAE\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 512, 512, 1)       0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 16), (None, 16),  8417248   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 512, 512, 1)       1175453   \n",
      "=================================================================\n",
      "Total params: 9,592,701\n",
      "Trainable params: 9,592,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Instatiating the models - gluing it all together\n",
    "encoder_model, intermediate_shape = encoder(**params)\n",
    "decoder_model = decoder(encoder_model.outputs[2], intermediate_shape, **params)\n",
    "\n",
    "outputs = decoder_model(encoder_model(encoder_model.inputs)[2])\n",
    "\n",
    "vae = Model(encoder_model.inputs, outputs, name = 'vae')\n",
    "\n",
    "print('ENCODER')\n",
    "print(encoder_model.summary())\n",
    "\n",
    "print('\\nDECODER')\n",
    "print(decoder_model.summary())\n",
    "\n",
    "print('\\nVAE')\n",
    "print(vae.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import mse\n",
    "\n",
    "# Adding loss - both (scaled) reconstruction and KL\n",
    "\n",
    "reconstruction_loss = mse(K.flatten(encoder_model.inputs), K.flatten(outputs))\n",
    "reconstruction_loss *= params['input_shape'][0]*params['input_shape'][0]\n",
    "kl_loss = 1. + encoder_model.outputs[1] - K.square(encoder_model.outputs[0]) - K.exp(encoder_model.outputs[1])\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonas/miniconda3/envs/hackathon/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "# Now it's time to train the model.\n",
    "# Let's train it with the adam optimizer\n",
    "\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# You can change these parameters around, especially the batch_size.\n",
    "# Remember, the images are large (512x512), so if the training crashes,\n",
    "# you more than likely have to reduce the batch size. If it still crashes\n",
    "# well.. You can modify the network to be smaller.\n",
    "# You can start with 4 filters, for example.\n",
    "# Or, you could get a GPU with more memory.\n",
    "# Or rent TPU server from Amazon or google\n",
    "training_params = {\n",
    "    'epochs': 10,\n",
    "    'batch_size': 2,\n",
    "    'shuffle': True,\n",
    "}\n",
    "\n",
    "vae.fit(x_train, **training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Put up some examples - Train, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
